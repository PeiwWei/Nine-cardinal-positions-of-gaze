{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e3def3-9f20-4fa9-bfb3-23583ea59a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--num_classes NUM_CLASSES] [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--lr LR]\n",
      "                             [--lrf LRF] [--data-path DATA_PATH] [--model-name MODEL_NAME] [--weights WEIGHTS]\n",
      "                             [--freeze-layers FREEZE_LAYERS] [--device DEVICE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Administrator\\AppData\\Roaming\\jupyter\\runtime\\kernel-dbd4f41a-3613-43f3-8f77-7baa107e9201.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda\\envs\\3.9\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from calflops import calculate_flops\n",
    "\n",
    "from my_dataset import MyDataSet\n",
    "# from vit_model import vit_base_patch16_224_in21k as create_model\n",
    "from model.VGG16.model import vgg as create_model\n",
    "from utils import read_split_data, train_one_epoch, evaluate\n",
    "\n",
    "from sklearn import metrics\n",
    "import copy\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if os.path.exists(\"./weights\") is False:\n",
    "        os.makedirs(\"./weights\")\n",
    "\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    train_images_path, train_images_label, val_images_path, val_images_label, test_images_path, test_images_label = read_split_data(args.data_path)\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.Resize((192, 576)),\n",
    "                                     transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "        \"val\": transforms.Compose([transforms.Resize((192, 576)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "        \"test\": transforms.Compose([transforms.Resize((192, 576)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])}\n",
    "\n",
    "    # 实例化训练数据集\n",
    "\n",
    "\n",
    "    test_dataset = MyDataSet(images_path=test_images_path,\n",
    "                            images_class=test_images_label,\n",
    "                            transform=data_transform[\"test\"])\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True,\n",
    "                                             num_workers=nw,\n",
    "                                             collate_fn=test_dataset.collate_fn)\n",
    "\n",
    "    model = create_model(num_classes=3,init_weights=True).to(device)\n",
    "    model_weight_path = \"./weights/vgg.pkl\"\n",
    "    state = torch.load(model_weight_path, map_location=device)\n",
    "\n",
    "    # new_key = 'encoder.layers.0.self_attn_long.self_attn_long.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_long.attn_long.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_long.self_attn_long.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_long.attn_long.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_long.self_attn_long.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_long.attn_long.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_long.self_attn_long.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_long.attn_long.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_short.self_attn_short.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_short.attn_short.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_short.self_attn_short.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_short.attn_short.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_short.self_attn_short.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_short.attn_short.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.0.self_attn_short.self_attn_short.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.0.self_attn_short.attn_short.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_long.self_attn_long.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_long.attn_long.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_long.self_attn_long.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_long.attn_long.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_long.self_attn_long.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_long.attn_long.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_long.self_attn_long.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_long.attn_long.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_short.self_attn_short.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_short.attn_short.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_short.self_attn_short.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_short.attn_short.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_short.self_attn_short.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_short.attn_short.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.1.self_attn_short.self_attn_short.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.1.self_attn_short.attn_short.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_long.self_attn_long.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_long.attn_long.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_long.self_attn_long.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_long.attn_long.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_long.self_attn_long.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_long.attn_long.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_long.self_attn_long.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_long.attn_long.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_short.self_attn_short.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_short.attn_short.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_short.self_attn_short.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_short.attn_short.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_short.self_attn_short.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_short.attn_short.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.2.self_attn_short.self_attn_short.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.2.self_attn_short.attn_short.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_long.self_attn_long.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_long.attn_long.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_long.self_attn_long.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_long.attn_long.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_long.self_attn_long.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_long.attn_long.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_long.self_attn_long.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_long.attn_long.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_short.self_attn_short.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_short.attn_short.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_short.self_attn_short.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_short.attn_short.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_short.self_attn_short.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_short.attn_short.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.3.self_attn_short.self_attn_short.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.3.self_attn_short.attn_short.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_long.self_attn_long.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_long.attn_long.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_long.self_attn_long.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_long.attn_long.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_long.self_attn_long.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_long.attn_long.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_long.self_attn_long.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_long.attn_long.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_short.self_attn_short.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_short.attn_short.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_short.self_attn_short.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_short.attn_short.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_short.self_attn_short.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_short.attn_short.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.4.self_attn_short.self_attn_short.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.4.self_attn_short.attn_short.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_long.self_attn_long.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_long.attn_long.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_long.self_attn_long.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_long.attn_long.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_long.self_attn_long.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_long.attn_long.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_long.self_attn_long.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_long.attn_long.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_short.self_attn_short.in_proj_weight'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_short.attn_short.in_proj_weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_short.self_attn_short.in_proj_bias'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_short.attn_short.in_proj_bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_short.self_attn_short.out_proj.weight'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_short.attn_short.out_proj.weight')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "    # new_key = 'encoder.layers.5.self_attn_short.self_attn_short.out_proj.bias'\n",
    "    # value = state.pop('encoder.layers.5.self_attn_short.attn_short.out_proj.bias')\n",
    "    # new_key_value_pair = {new_key: value}\n",
    "    # state.update(new_key_value_pair)\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    # batch_size = 1\n",
    "    # input_shape = (batch_size, 3, 192, 576)\n",
    "    # flops, macs, params = calculate_flops(model=model,\n",
    "    #                                       input_shape=input_shape,\n",
    "    #                                       output_as_string=True,\n",
    "    #                                       output_precision=5)\n",
    "    # print(\"FLOPs:%s   MACs:%s   Params:%s \\n\" % (flops, macs, params))\n",
    "\n",
    "    best_acc = 0.0\n",
    "    print(best_acc)\n",
    "    test_loss, test_acc , test_acc_1, test_acc_2 ,sensitivity, specificity, precision, f1_score= evaluate(model=model,\n",
    "                                 data_loader=test_loader,\n",
    "                                 device=device,\n",
    "                                 epoch=0)\n",
    "    print('result_acc_test: ', test_acc)\n",
    "    print('result_acc_test_1: ', test_acc_1)\n",
    "    print('result_acc_test_2: ', test_acc_2)\n",
    "    print('result_sensitivity: ', sensitivity)\n",
    "    print('result_specificity: ', specificity)\n",
    "    print('result_precision: ', precision)\n",
    "    print('result_f1_score: ', f1_score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_classes', type=int, default=3)\n",
    "    parser.add_argument('--epochs', type=int, default=500)\n",
    "    parser.add_argument('--batch-size', type=int, default=1)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--lrf', type=float, default=0.01)\n",
    "\n",
    "    # 数据集所在根目录\n",
    "    parser.add_argument('--data-path', type=str,\n",
    "                        default=\"./data\")\n",
    "    parser.add_argument('--model-name', default='', help='create model name')\n",
    "\n",
    "    # 预训练权重路径，如果不想载入就设置为空字符\n",
    "    parser.add_argument('--weights', type=str, default='',\n",
    "                        help='initial weights path')\n",
    "    # ./vit_base_patch16_224_in21k.pth\n",
    "    # 是否冻结权重\n",
    "    # parser.add_argument('--freeze-layers', type=bool, default=True)\n",
    "    parser.add_argument('--freeze-layers', type=bool, default=False)\n",
    "    parser.add_argument('--device', default='cuda:0', help='device id (i.e. 0 or 0,1 or cpu)')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "\n",
    "    main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7952fca-0933-4107-b9a4-156ef793bbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9",
   "language": "python",
   "name": "3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
